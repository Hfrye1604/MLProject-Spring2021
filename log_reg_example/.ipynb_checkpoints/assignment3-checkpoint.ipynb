{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS596 Machine Learning \n",
    "# Assignment 3: Logistic Regression\n",
    "\n",
    "### Due 11:59 pm, Friday, 2/19/2021\n",
    "\n",
    "### Total: 15 points\n",
    "\n",
    "In this assignment, we will train a logistic regression model that classifies two categories of sign language images. The original data can be obtained from https://www.kaggle.com/ardamavi/sign-language-digits-dataset (preprocessed) or https://github.com/ardamavi/Sign-Language-Digits-Dataset (raw).\n",
    "\n",
    "Your goal is to run all the cells below one by one from top to bottom. Before you run some task cells, you need to complete the missing lines (notified by \"= None\" in Python) in them. \n",
    "\n",
    "For each **task** cell that requires your completion, you can run the **evaluation** cell right after it to check if your answer correct.\n",
    "The output of the evaluation cell should be the same as the \"expected output\" provided. (Some mismatch in the last digit of floating numbers is tolerable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE: Make sure you run `pip install -U scikit-image` in your command line \n",
    "# to have the newest version of scikit-image installed.\n",
    "from skimage.io import imsave \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and preprocess\n",
    "\n",
    "First, you need to make sure that the **sign-language-digits-dataset.zip** file is downloaded and unzipped to the same folder of this assignment3.ipynb file.\n",
    "\n",
    "The original data have 10 categories, labelled by digit 0 to 9. \n",
    "We choose the image of sign ***three*** and ***four*** as the two categories to train our binary classifier.\n",
    "The label for sign \"three\" is 0, and the label for \"four\" is 1.\n",
    "\n",
    "Run the next cell (**DO NOT** change the code), and it will prepare you the training and testing data needed for the assignment. This part is not graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image data and preprocess\n",
    "X_raw = np.load(open('X.npy', 'rb'))\n",
    "Y_raw = np.load(open('y.npy', 'rb'))\n",
    "print('X_raw shape: {}'.format(X_raw.shape))\n",
    "print('Y_raw shape: {}'.format(Y_raw.shape))\n",
    "\n",
    "# Flatten X_raw and transpose\n",
    "X_data = X_raw.reshape(X_raw.shape[0], -1).T\n",
    "print('X_data shape: {}'.format(X_data.shape))\n",
    "\n",
    "# Trnasponse Y_raw and convert from one-hot labels to integer labels\n",
    "Y_data = Y_raw.T\n",
    "Y_data = np.argmax(Y_data, axis=0).reshape((1, Y_data.shape[1]))\n",
    "print('Y_data shape: {}'.format(Y_data.shape))\n",
    "\n",
    "# Choose the data of first two categories (images for number \"three\" and \"four\") \n",
    "X_cat0 = X_data[:, np.where(Y_data == 0)[1]]\n",
    "Y_cat0 = Y_data[:, np.where(Y_data == 0)[1]]\n",
    "X_cat1 = X_data[:, np.where(Y_data == 6)[1]]\n",
    "Y_cat1 = Y_data[:, np.where(Y_data == 6)[1]]\n",
    "\n",
    "# Convert the label of Y_cat0 to 0, and Y_cat1 to 1\n",
    "Y_cat0 = np.zeros_like(Y_cat0)\n",
    "Y_cat1 = np.ones_like(Y_cat1)\n",
    "\n",
    "print()\n",
    "print('X_cat0 shape: {}'.format(X_cat0.shape))\n",
    "print('Y_cat0 shape: {}'.format(Y_cat0.shape))\n",
    "print('X_cat1 shape: {}'.format(X_cat1.shape))\n",
    "print('Y_cat1 shape: {}'.format(Y_cat1.shape))\n",
    "\n",
    "# Select the first 70% from each category and combine them together as training data, and the rest as test data\n",
    "ind_cat0 = int(0.7 * X_cat0.shape[1])\n",
    "ind_cat1 = int(0.7 * X_cat1.shape[1])\n",
    "\n",
    "X_train = np.concatenate((X_cat0[:, :ind_cat0], X_cat1[:, :ind_cat1]), axis=1)\n",
    "Y_train = np.concatenate((Y_cat0[:, :ind_cat0], Y_cat1[:, :ind_cat1]), axis=1)\n",
    "X_test = np.concatenate((X_cat0[:, ind_cat0:], X_cat1[:, ind_cat1:]), axis=1)\n",
    "Y_test = np.concatenate((Y_cat0[:, ind_cat0:], Y_cat1[:, ind_cat1:]), axis=1)\n",
    "\n",
    "print()\n",
    "print('X_train shape {}'.format(X_train.shape))\n",
    "print('Y_train shape {}'.format(Y_train.shape))\n",
    "print('X_test shape {}'.format(X_test.shape))\n",
    "print('Y_test shape {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "$X_{train}$ is a $4096\\times 286$ numpy array. $Y_{train}$ is a $1\\times 286$. They are the train data.\n",
    "\n",
    "$X_{test}$ is a $4096\\times 125$. $Y_{test}$ is a $1\\times 125$. They are the test data.\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|X_raw shape: |(2062, 64, 64)|\n",
    "|Y_raw shape: |(2062, 10)\n",
    "|X_data shape: |(4096, 2062)\n",
    "|Y_data shape: |(1, 2062)\n",
    "|--|--|\n",
    "X_cat0 shape: |(4096, 204)\n",
    "Y_cat0 shape: |(1, 204)\n",
    "X_cat1 shape: |(4096, 207)\n",
    "Y_cat1 shape: |(1, 207)\n",
    "|--|--|\n",
    "X_train shape |(4096, 286)\n",
    "Y_train shape |(1, 286)\n",
    "X_test shape |(4096, 125)\n",
    "Y_test shape |(1, 125)\n",
    "\n",
    "\n",
    "# Visualize\n",
    "\n",
    "The following cell will plot two examples of category 0 (number \"three\") and 1 (number \"four\"), which gives you a sense of the data. This part is not graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize some data\n",
    "img_cat0 = X_cat0[:, 14].reshape((64,64))\n",
    "img_cat1 = X_cat1[:, 14].reshape((64,64))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(img_cat0)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(img_cat1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1. Implement sigmoid function\n",
    "\n",
    "**1.5 points**\n",
    "\n",
    "Implement the sigmoid function \n",
    "$a = \\sigma(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "\n",
    "    Arg:\n",
    "    z -- A number or numpy array\n",
    "\n",
    "    Return:\n",
    "    a -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    a = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 1\n",
    "print('sigmoid(-10) = {}'.format(sigmoid(-10)))\n",
    "print('sigmoid(10) = {}'.format(sigmoid(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|:--:|\n",
    "|**sigmoid(-10) =**|4.5397868702434395e-05|\n",
    "|**sigmoid(10) =** |0.9999546021312976|\n",
    "\n",
    "---\n",
    "\n",
    "# Task 2. Initialize parameters\n",
    "**1.5 points**\n",
    "\n",
    "Implement the function that returns parameter $w$ and $b$ with zero values. The dimension of $w$ is given by the argument *dim*, i.e., it should be a $dim\\times 1$ vector. Note that in numpy the dimension of $w$ should be `(dim, 1)`, but not `(dim,)`.\n",
    "\n",
    "*Hint:* use np.zeros to initialize $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "def init_zeros(dim):\n",
    "    \"\"\"\n",
    "    Initialize parameters w, b to zeros\n",
    "    \n",
    "    Arg:\n",
    "    dim -- size of w \n",
    "    \n",
    "    Returns:\n",
    "    w -- a numpy vector of shape (dim, 1)\n",
    "    b -- a number\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    w = None\n",
    "    b = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 2\n",
    "dim = 3\n",
    "w, b = init_zeros(dim)\n",
    "print('w = {}'.format(w))\n",
    "print('b = {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**w =**|[[0.] <br> [0.] <br> [0.]]\n",
    "**b =**|0.0\n",
    "\n",
    "---\n",
    "# Task 3. Implement forward and backward computation\n",
    "**3 points**\n",
    "\n",
    "The following function conducts the forward and backward computation for logistic regression.\n",
    "\n",
    "Given input data $X$ and label $Y$, parameters $w$ and $b$, it returns the gradients $dw$ and $db$, and the cost as well.\n",
    "\n",
    "In the forward pass:\n",
    "- The activation is computed by: $A=\\sigma(w^{T}X + b)$\n",
    "- Cost is computed by the formula: $-\\frac{1}{m} \\sum_i^{m}[y^{(i)}\\log(a^{(i)}) + (1 - y^{(i)})\\log(1 - a^{(i)})]$\n",
    "\n",
    "In the backward pass:\n",
    "- $dZ = A - Y$\n",
    "- $dw = \\frac{1}{m}XdZ^{T}$\n",
    "- $db = \\frac{1}{m}\\text{np.sum}(dZ)$\n",
    "\n",
    "The gradients are returned in a Python dict object, whose keys are \"dw\" and \"db\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "def forward_backward(X, Y, w, b):\n",
    "    \"\"\"\n",
    "    Implement the forward and backward passes for logistic regression\n",
    "    \n",
    "    Args:\n",
    "    X -- data of size (64 * 64, number of examples)\n",
    "    Y -- true label vector of size (1, number of examples)\n",
    "    w -- weights, a numpy array of size (64 * 64, 1)\n",
    "    b -- bias, a scalar\n",
    "    \n",
    "    Returns:\n",
    "    grads -- containing gradients, dw and db\n",
    "    cost -- cost for the current pass\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward\n",
    "    ### START YOUR CODE ###\n",
    "    A = None\n",
    "    cost = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    # Backward\n",
    "    ### START YOUR CODE ###\n",
    "    dZ = None\n",
    "    dw = None\n",
    "    db = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 3\n",
    "X = np.array([[1,2,-3,0],[0.5,6,-5,0]])\n",
    "Y = np.array([[1,0,1,0]])\n",
    "w = np.array([[1],[2]])\n",
    "b = 0\n",
    "grads, cost = forward_backward(X, Y, w, b)\n",
    "\n",
    "print('dw = {}'.format(grads['dw']))\n",
    "print('db = {}'.format(grads['db']))\n",
    "print('cost = {}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**dw =**|[[1.22019716] <br> [2.73509556]]\n",
    "**db =**| 0.09519962669353813\n",
    "**cost =**| 6.9550195708335805\n",
    "\n",
    "---\n",
    "# Task 4. Implement gradient descent.\n",
    "\n",
    "**2 points**\n",
    "\n",
    "In GD function, call `forward_backward()` function for `num_iters` times. Within each iterationl, parameters $w$ and $b$ are updated. The final parameters and a list of cost values are returned. \n",
    "\n",
    "*Hint: parameters are updated by $w = w - \\alpha * dw$* and $b = b - \\alpha * db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "def GD(X, Y, w, b, num_iters, alpha, verbose=False):\n",
    "    \"\"\"\n",
    "    Implement gradient descent\n",
    "    \n",
    "    Args:\n",
    "    X -- data of size (64 * 64, number of examples)\n",
    "    Y -- true label vector of size (1, number of examples)\n",
    "    w -- weights, a numpy array of size (64 * 64, 1)\n",
    "    b -- bias, a scalar\n",
    "    num_iters -- number of iterations\n",
    "    alpha -- learning rate\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    costs -- list of all the costs computed during the training, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Compute gradient and cost by calling forward_backward function\n",
    "        ##### START YOUR CODE #####\n",
    "        grads, cost = None\n",
    "        ##### END YOUR CODE #####\n",
    "        \n",
    "        # Obtain dw and db\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        # Update parameters\n",
    "        ##### START YOUR CODE #####\n",
    "        w = None\n",
    "        b = None\n",
    "        ##### END YOUR CODE #####\n",
    "        \n",
    "        # Record and print cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if i % 100 == 0 and verbose:\n",
    "            print(\"Cost after iter {}: {}\".format(i, cost))\n",
    "        \n",
    "    params = {'w': w, 'b': b}\n",
    "        \n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 4\n",
    "params, costs = GD(X, Y, w, b, num_iters=1000, alpha=0.01)\n",
    "print('w = {}'.format(params['w']))\n",
    "print('b = {}'.format(params['b']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**w =**|[[ 0.57327302] <br> [-0.8933432 ]]\n",
    "**b =**| 0.05089921193049401\n",
    "\n",
    "---\n",
    "# Task 5. Implement the function to predict\n",
    "\n",
    "**2 points**\n",
    "\n",
    "Given new data $X$, parameters $w$ and $b$, the function makes predictions on whether each example in $X$ is 0 or 1.\n",
    "\n",
    "*Hint*:\n",
    "- First, compute the activation using $A = \\sigma(w^{T}X + b)$. The resulting $A$ is a $1\\times m$ matrix, $[a^{(1)},a^{(2)},\\dots,a^{(m)}]$, in which $a^{(i)}$ is the probability that $x^{(i)}=1$.\n",
    "- Second, you need to convert probabilities to real predictions by assigning $Y_{pred}^{(i)}$ to 1 if $a^{(1)}>0.5$, else to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Implement predict function\n",
    "    \n",
    "    Args:\n",
    "    X -- data of size (64 * 64, number of examples)\n",
    "    w -- weights, a numpy array of size (64 * 64, 1)\n",
    "    b -- bias, a scalar\n",
    "    \n",
    "    Returns:\n",
    "    Y_pred -- a numpy array of size (1, number of examples) containing all predictions (0/1) for all the examples in X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    Y_pred = np.zeros((1, m))\n",
    "    \n",
    "    # Compute the activation A\n",
    "    ##### START YOUR CODE #####\n",
    "    A = None\n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    for i in range(A.shape[1]):\n",
    "        ##### START TODO #####\n",
    "        Y_pred[0, i] = None\n",
    "        ##### END TODO #####\n",
    "    \n",
    "    assert(Y_pred.shape == (1, m))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 5\n",
    "print('predictions = {}'.format(predict(X, w, b)))\n",
    "print('predictions = {}'.format(predict(X, params['w'], params['b'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**predictions** | [[1. 1. 0. 0.]]\n",
    "**predictions** | [[1. 0. 1. 1.]]\n",
    "\n",
    "\n",
    "---\n",
    "# Task 6. Integrate into one model\n",
    "\n",
    "**3 points**\n",
    "\n",
    "Integrate the above parts into one model, and apply the model on the train data ($X_{train}, Y_{train}$), and then evaluate on test data ($X_{test},Y_{test}$).\n",
    "\n",
    "*Hint:*\n",
    "- You need to call init_zeros and GD in order. You should pass the initialized parameters and all the other necessary arguments to GD.\n",
    "- You also need to compute the accuray on train and test data respectively. Accuray is defined by the fraction of correct predictions over total number of examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iters=2000, alpha=0.005, verbose=False):\n",
    "    \"\"\"\n",
    "    Integrated model\n",
    "    \n",
    "    Args:\n",
    "    X_train -- training data of size (4096, 286)\n",
    "    Y_train -- training label of size (1, 286)\n",
    "    X_test -- test data of size (4096, 125)\n",
    "    Y_test -- test label of size (1, 125)\n",
    "    \n",
    "    Returns:\n",
    "    result -- a dict object that contains useful information\n",
    "    \"\"\"\n",
    "    ##### START YOUR CODE #####\n",
    "    # Initialize parameters to zeros\n",
    "    w, b = None\n",
    "    \n",
    "    # Conduct gradient descent\n",
    "    params, costs = None\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # Use the parameters to predict on train and test data\n",
    "    Y_pred_train = None\n",
    "    Y_pred_test = None\n",
    "    \n",
    "    # Compute the accuracies of predicting on train/test data\n",
    "    # Accuracy is the fraction of correct predictions over all examples\n",
    "    acc_train = None\n",
    "    acc_test = None\n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    # Print train/test accuracies\n",
    "    print('train accuracy: {} %'.format(100 * acc_train))\n",
    "    print('test accuracy: {} %'.format(100 * acc_test))\n",
    "    \n",
    "    result = {\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "        'costs': costs,\n",
    "        'Y_pred_test': Y_pred_test\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 6\n",
    "res = model(X_train, Y_train, X_test, Y_test, num_iters=1500, alpha=0.002, verbose=True)\n",
    "\n",
    "# Plot learning curve\n",
    "costs = np.squeeze(res['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Learning rate = 0.002')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Cost after iter 0: |0.6931471805599454\n",
    "Cost after iter 100: |0.5946773825987639\n",
    "Cost after iter 200: |0.5256364501984687\n",
    "Cost after iter 300: |0.4747208768166399\n",
    "Cost after iter 400: |0.435436416758632\n",
    "Cost after iter 500: |0.40399872095331557\n",
    "Cost after iter 600: |0.37811027839268685\n",
    "Cost after iter 700: |0.35630887692114865\n",
    "Cost after iter 800: |0.3376209341419335\n",
    "Cost after iter 900: |0.32137148224069756\n",
    "Cost after iter 1000: |0.30707586651947666\n",
    "Cost after iter 1100: |0.29437547177794215\n",
    "Cost after iter 1200: |0.28299807348845724\n",
    "Cost after iter 1300: |0.27273248705908887\n",
    "Cost after iter 1400: |0.26341182071904296\n",
    "train accuracy: | 94.05594405594405 %\n",
    "test accuracy: | 88.0 %\n",
    "\n",
    "<br>\n",
    "<img src=\"lc.png\">\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Task 7. Calculate evaluation metrics\n",
    "\n",
    "**2 points**\n",
    "\n",
    "Calculate 8 evaluation metrics out of the previous results stored in the \"res\" object, using the ground truth label $Y_{test}$ and the predictions on $Y_{test}$, which is stored in res['Y_pred_test'].\n",
    "\n",
    "**NOTE**: We assumte that label y = 1 is positive, and y = 0 is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.\n",
    "# Calculate TP, FP, TN, FN, Accuracy, Precision, Recall, and F-1 score\n",
    "# We assume that label y = 1 is positive, and y = 0 is negative\n",
    "def calc_metrics(Y_test, Y_pred_test):\n",
    "    \"\"\"\n",
    "    Calculate metrics\n",
    "    \n",
    "    Args:\n",
    "    Y_test -- test label\n",
    "    Y_pred_test -- predictions on test data\n",
    "    \n",
    "    Return:\n",
    "    metrics -- a dict object\n",
    "    \"\"\"\n",
    "    assert(Y_test.shape == Y_pred_test.shape)\n",
    "    \n",
    "    ##### START YOUR CODE #####\n",
    "    TP = None\n",
    "    FP = None\n",
    "    TN = None\n",
    "    FN = None\n",
    "    Accuracy = None\n",
    "    Precision = None\n",
    "    Recall = None\n",
    "    F1 = None\n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    metrics = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN,\n",
    "        'Accuracy': Accuracy,\n",
    "        'Precision': Precision,\n",
    "        'Recall': Recall,\n",
    "        'F1': F1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 7\n",
    "m = calc_metrics(Y_test, res['Y_pred_test'])\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {}, \\nAccuracy = {}, Precision = {}, Recall = {}, F1 = {}'.format(\n",
    "    m['TP'], m['FP'], m['TN'], m['FN'], m['Accuracy'], m['Precision'], m['Recall'], m['F1']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;|&nbsp;|&nbsp;\n",
    "--|--|--|--\n",
    "TP = 59 | FP = 11 | TN = 51 | FN = 4\n",
    "Accuracy = 0.88 | Precision = 0.8428571428571429 | Recall = 0.9365079365079365 | F1 = 0.887218045112782"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
